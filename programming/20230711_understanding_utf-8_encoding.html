<!--(11-July-2023) /programming/understanding_utf-8_encoding -->

<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>Understanding UTF-8 Encoding</title>
        <link rel="stylesheet" type="text/css" href="../styles.css">
    </head>

    <body>
        <h1><a href="../index.html">..</a></h1>
        <h1>(11-July-2023) Understanding UTF-8 Encoding</h1>
        <!-- <p class="text-container"></p> -->
        <!-- <a href="" target="_blank"></a> -->
        <p class="text-container">Honestly, I'm a bit ashamed I only looked into this today, but hey, better late than never :P.</p>
        <p class="text-container">So today I learned about <a href="https://pt.wikipedia.org/wiki/UTF-8" target="_blank">UTF-8</a> in the detail I should have learned about when I first heard about UTF-8. Let's begin by understanding what I previously thought UTF-8 was and how I thought it looked like in memory.</p>
        <p class="text-container">To me, UTF-8 was Unicode. Yep. I really did thought about UTF-8 as Unicode, and the reason why I thought about it like that was that everytime I have trouble displaying Unicode characters (at least the ones outside of the ASCII table), I would hear about how "You need to use UTF-8", so that's where the association began. I also thought that UTF-8 was 2 bytes fixed size per character (the 8 in the end did not ring any bells in my head at the time), because in a college class I had, using the Windows API with the C programming language, wide characters where 2 bytes long, and the way to figure out if the program was using wide chars was with a preprocessor #ifdef UNICODE directive, and since Unicode and UTF-8 where the same thing in my head, I just assumed UTF-8 = 2 bytes. I feel a bit silly now that I understand what actually is going on.</p>
        <p class="text-container">UTF-8 is really just a way to have Unicode characters, but in a compact manner. It's not Unicode exactly, it's just a way to format Unicode characters (hence the name UTF - Unicode Transformation Format). UTF-16 serves the same purpose, it just uses different algorithms. Regarding the number (8 in UTF-8 and 16 in UTF-16), it's there to represent the minimum number of bits that it takes to represent a character. In UTF-8 it would be 8 bits (aka 1 byte) and UTF-16 it would be 16 bits (aka 2 bytes). UTF-8 can grow from 1 byte up to 4 bytes, depending on the character it's trying to format, and UTF-16 can use either 16 bits (2 bytes) or 32 bits (4 bytes) for the same purpose.</p>
        <p class="text-container">After understanding this, my first question was "So how would I even know how many bits are used for a character, as I'm reading it? There must be some sort of metadata being stored somewhere, else it would be impossible to understand what is what!", and indeed, there sort-of is. The way UTF-8 "hides" the information about each character, is in the beginning bits of each byte. This tells us, if it's the first byte of a character, how many bytes it has (how many bytes it used to represent that character), or if it's not the first byte, it has a specific pattern of bits to indicate that. This is all the information we need to understand (and even to convert to) UTF-8!</p>
        <p class="text-container">Here is how the bits are used: (Assuming we are on the first byte of a character) If the byte starts with a 0XXXXXXX, then it is a 7-bit ASCII character (U+0000 to U+007F); If the byte starts with 110XXXXX, then it is the first byte of a 2 bytes character (U+0080 to U+07FF); If the byte starts with 1110XXXX, we are looking at the first byte of a 3 bytes character (U+0800 to U+FFFF); If the byte starts with 11110XXX then we are at the first byte of a 4 bytes character (U+10000 to U+10FFFF). If we are not looking at the first byte of a characters, and looking at a byte that is part of the bytes of a character, these will start with 10XXXXXX.</p>
        <p class="text-container">The fun part about this was that someone at work was explaining some server-side issues regarding character encoding. The issue was that SQL Server stored characters as UTF-8, and in the database we had some fields for strings that accepted up to 50 bytes (yes, bytes, not characters). This means that we need to be careful storing text since some characters might be larger than 1 byte, so limiting the input fields to 50 characters was not safe. The embarrassing part was, the way I previously though of UTF-8 didn't have this issue (I thought UTF-8 was 2 bytes fixed size), so we went on and on a bit around this because I failed to understand the issue. Luckily, I figured out by myself (because after that talk I had to fact check what I was saying to be sure, since I was starting to question my understanding on the topic) all about UTF-8, and then went and told the person I talked with that I was actually wrong, and learned new stuff (more importantly, corrected previous assumed knowledge) along the way. This is why it's important to talk with others about programming and discuss things. Have a nice day!</p>
    </body>
</html>